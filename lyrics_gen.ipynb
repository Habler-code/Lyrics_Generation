{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pretty_midi\n",
    "import glob\n",
    "import time\n",
    "import random\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.utils.class_weight import compute_class_weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras import Input, Sequential, Model, initializers\n",
    "from tensorflow.keras.layers import Flatten, Dense, Lambda, InputLayer, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, Adagrad, Adadelta , RMSprop\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Activation\n",
    "from tensorflow.keras.callbacks import EarlyStopping,LambdaCallback, TensorBoard\n",
    "from tensorflow.keras.models import load_model\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#from similarity.normalized_levenshtein import NormalizedLevenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = r'C:\\Users\\idan\\Desktop\\ass3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Process songs from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pickle(path):\n",
    "    try:\n",
    "        pick = pickle.load(open(path, \"rb\"))\n",
    "        print(\"loaded pickle successfully\")\n",
    "        return pick\n",
    "    except (OSError, IOError) as e:\n",
    "        print(e)\n",
    "        return 0\n",
    "def dump_pickle(path, output):\n",
    "    print(\"dumping to pickle, path {}\".format(path))\n",
    "    pick = open(path, \"wb\")\n",
    "    pickle.dump(output, pick)\n",
    "    pick.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_language(path):\n",
    "    #return a set of all words withing the entire songs\n",
    "    vocabulary = []\n",
    "    df = pd.read_csv(path,header=None)\n",
    "    lyrics = df[:][2]\n",
    "    lyrics = lyrics.ravel()\n",
    "    for song_words in lyrics:\n",
    "        songs_words = song_words.split(' ')\n",
    "        #remove all chars which are not alphanumeric\n",
    "        pattern = re.compile('[^\\w&]+')\n",
    "        res = [re.sub(pattern, '', word) for word in songs_words]\n",
    "        vocabulary.extend(res)\n",
    "    vocabulary = set(vocabulary)\n",
    "    try:\n",
    "        vocabulary.remove('')\n",
    "        vocabulary.remove(' ')\n",
    "    except Exception:\n",
    "        pass\n",
    "    return vocabulary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_songs_words(path):\n",
    "    #returns a dict of song name and its words\n",
    "    df = pd.read_csv(path,header = None)\n",
    "    songs_dict = {}\n",
    "    songs = df[:][1]\n",
    "    pattern = re.compile('[^\\w&]+')\n",
    "    for song_name in songs:\n",
    "        words = df[df[1] == song_name][2].values[0]\n",
    "        songs_words = words.split(' ')\n",
    "        #remove all chars which are not alphanumeric\n",
    "        words = [re.sub(pattern, '', word) for word in songs_words]\n",
    "        words = list(filter(('').__ne__, words))\n",
    "        words = list(filter((' ').__ne__, words))\n",
    "        songs_dict[song_name.strip()] = words\n",
    "    return songs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_embeddings(apiwords):\n",
    "    E = api.load(apiwords)\n",
    "    return E\n",
    "\n",
    "def generate_onehot_for_language(L):\n",
    "    onehot_dict = {}\n",
    "    #each word into number\n",
    "    le = LabelEncoder()\n",
    "    transofrmed_words = le.fit_transform(L)\n",
    "    transofrmed_words = transofrmed_words.reshape(transofrmed_words.shape[0],1)\n",
    "    #transofrmed_words shape is (size of language,1)\n",
    "    #each number into onehot vector \n",
    "    one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "    onehot_transformed = one_hot_encoder.fit_transform(transofrmed_words)\n",
    "    #onehot_transformed shape is (size of lang , size of lang) --> each word has 1 on the correct index reprsenting the word.\n",
    "    word_onehot_touples = zip(L, onehot_transformed)\n",
    "    for word, onehot in word_onehot_touples:\n",
    "        onehot_dict[word] = onehot\n",
    "    return onehot_dict\n",
    "\n",
    "def generate_one_hot_embedding(path):\n",
    "    #returns a dict of index to one-hot-representation\n",
    "    #i.e embedding[704]: [0....299]\n",
    "    output = r\"{}\\one_hot_embedded.pickle\".format(env)\n",
    "    embedding = read_pickle(output)\n",
    "    if embedding == 0:\n",
    "        language = extract_language(path)\n",
    "        api = r'word2vec-google-news-300'\n",
    "        E = download_embeddings(api)\n",
    "        L = np.array(list(language))\n",
    "        one_hot = generate_onehot_for_language(L)\n",
    "        embedding = {}\n",
    "        for word, onehotarray in one_hot.items():\n",
    "            index = np.argmax(onehotarray)\n",
    "            try:\n",
    "                embedding[index] = E[word]\n",
    "            except Exception:\n",
    "                #if there is no embedding for key in E\n",
    "                embedding[index] = np.zeros(300)\n",
    "        dump_pickle(output, embedding)\n",
    "    return embedding\n",
    "\n",
    "\n",
    "def generate_one_hot_words(path):\n",
    "    #returns a dict of index to word\n",
    "    #i.e embedding[704]: 'blues'\n",
    "    output = r\"{}\\one_hot_to_word.pickle\".format(env)\n",
    "    embedding = read_pickle(output)\n",
    "    if embedding == 0:\n",
    "        language = extract_language(path)\n",
    "        api = r'word2vec-google-news-300'\n",
    "        E = download_embeddings(api)\n",
    "        L = np.array(list(language))\n",
    "        one_hot = generate_onehot_for_language(L)\n",
    "        embedding = {}\n",
    "        for word, onehotarray in one_hot.items():\n",
    "            index = np.argmax(onehotarray)\n",
    "            embedding[index] = word\n",
    "        dump_pickle(output, embedding)\n",
    "    return embedding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = r'{}\\lyrics_train_set.csv'.format(env)\n",
    "\n",
    "env = r'C:\\Users\\idan\\Desktop\\ass3'\n",
    "#glove-wiki-gigaword-300 & ord2vec-google-news-300\n",
    "print('generate one hot embedding')\n",
    "one_hot_embedding = generate_one_hot_embedding(path)\n",
    "print('generate one hot words')\n",
    "one_hot_words = generate_one_hot_words(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process MIDI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_midi_vector(midi_info, song_length, strategy):\n",
    "    \"\"\"\n",
    "    returns normalized vector represents the melody of each song\n",
    "    \"\"\"\n",
    "    if strategy == 1:\n",
    "        words_to_create = 500\n",
    "        notes_per_word = 10\n",
    "        pitch_norm = 150\n",
    "        instruments_number = 10\n",
    "        instruments_list = midi_info.instruments\n",
    "        song = []\n",
    "        for word in range(song_length):\n",
    "            for instrument in instruments_list[:instruments_number]:\n",
    "                notes = instrument.notes\n",
    "                notes_counter = 0\n",
    "                notes_interval = notes[word * notes_per_word:(word+1)*notes_per_word]\n",
    "                for note in notes_interval:\n",
    "                    song.append(note.pitch) \n",
    "                    notes_counter = notes_counter + 1\n",
    "                if len(notes_interval) < notes_per_word:\n",
    "                    for i in range(notes_per_word - len(notes_interval)):\n",
    "                        song.append(0)\n",
    "        return np.array(song) / pitch_norm\n",
    "    elif strategy == 2:\n",
    "        chroma_norm = 500\n",
    "        et = midi_info.get_end_time()\n",
    "        chromas = midi_info.get_chroma(fs=et/song_length, times=np.arange(0, et,et/song_length)).T\n",
    "        chromas = chromas / chroma_norm\n",
    "        return chromas\n",
    "\n",
    "\n",
    "def generate_melody_vecors(path,output_name, strategy):\n",
    "    \"\"\"\n",
    "    return a dict of song name and correspondding melody vector representation\n",
    "    \"\"\"\n",
    "    embeddings = read_pickle(r'{}\\{}.pickle'.format(env,output_name))\n",
    "    if embeddings == 0:\n",
    "        midis = glob.glob(r\"{}\\midi_files\\*.mid\".format(env))\n",
    "        midis_songs_name = [f.split('_-_')[1].split('.')[0].lower().replace(\"_\",\" \") for f in midis]\n",
    "        songs = get_songs_words(path)\n",
    "        embeddings = {}\n",
    "        for song_name, song_words in songs.items():\n",
    "            if song_name in midis_songs_name:\n",
    "                try:\n",
    "                    index = midis_songs_name.index(song_name)\n",
    "                    midi_path = midis[index]\n",
    "                    midi_info = pretty_midi.PrettyMIDI(r'{}'.format(midi_path))\n",
    "                    song_length = len(song_words)\n",
    "                    embeddings[song_name] = get_midi_vector(midi_info, song_length, strategy)\n",
    "                except:\n",
    "                    print('error while loading midi file - {}'.format(midi_path))  \n",
    "            else:\n",
    "                print('could not find midi file for {}'.format(song_name))\n",
    "        dump_pickle(r'{}\\{}.pickle'.format(env,output_name),embeddings)\n",
    "    return embeddings\n",
    "            \n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = 'C:\\\\Users\\\\idan\\\\Desktop\\\\ass3'\n",
    "path = r'{}\\lyrics_train_set.csv'.format(env)  \n",
    "trian_melody_embeddings_s1= generate_melody_vecors(path, 'midi_train_set_S1',1)\n",
    "path = r'{}\\lyrics_test_set.csv'.format(env)  \n",
    "test_melody_embeddings_s1 = generate_melody_vecors(path, 'midi_test_set_S1',1)\n",
    "melody_embeddings_s1 = {}\n",
    "melody_embeddings_s1.update(trian_melody_embeddings_s1)\n",
    "melody_embeddings_s1.update(test_melody_embeddings_s1)\n",
    "dump_pickle(r'{}\\{}.pickle'.format(env,'all_melodies_S1'), melody_embeddings_s1)\n",
    "\n",
    "\n",
    "path = r'{}\\lyrics_train_set.csv'.format(env)  \n",
    "trian_melody_embeddings_s2= generate_melody_vecors(path, 'midi_train_set_S2',2)\n",
    "path = r'{}\\lyrics_test_set.csv'.format(env)  \n",
    "test_melody_embeddings_s2 = generate_melody_vecors(path, 'midi_test_set_S2',2)\n",
    "melody_embeddings_s2 = {}\n",
    "melody_embeddings_s2.update(trian_melody_embeddings_s2)\n",
    "melody_embeddings_s2.update(test_melody_embeddings_s2)\n",
    "dump_pickle(r'{}\\{}.pickle'.format(env,'all_melodies_S2'), melody_embeddings_s2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def add_melody_for_word(word_embedding, melodies,index,strategy):\n",
    "    if strategy == 1:\n",
    "        notes_per_word = 10\n",
    "        instruments_number = 10\n",
    "        try:\n",
    "            word_melody = melodies[index * notes_per_word * instruments_number : (index + 1) * notes_per_word * instruments_number]\n",
    "            if len(word_melody) < notes_per_word * instruments_number:\n",
    "                word_melody = np.zeros(notes_per_word * instruments_number)\n",
    "            if len(word_embedding) < 300:\n",
    "                word_embedding = np.zeros(300)\n",
    "            return np.concatenate([word_embedding, word_melody])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return np.concatenate([word_embedding, np.zeros(notes_per_word * instruments_number)])\n",
    "    if strategy == 2:\n",
    "        try:\n",
    "            word_melody = melodies[index]\n",
    "            if len(word_melody) == 0:\n",
    "                word_melody = np.zeros(12)\n",
    "            if len(word_embedding) == 0:\n",
    "                word_embedding = np.zeros(300)\n",
    "            return np.concatenate([word_embedding, word_melody])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return np.concatenate([word_embedding, np.zeros(12)])        \n",
    "\n",
    "\n",
    "def generate_dataset(melodies, E, songsL, L, L_one_hot, strategy):\n",
    "    \"\"\"\n",
    "    generating dataset while X contains an embedding of a word + part of melody\n",
    "    Y contains the one-hot of the following word.\n",
    "    \"\"\"\n",
    "    songs = songsL.keys()\n",
    "    songs_X = []\n",
    "    songs_Y = []\n",
    "    for song in songs:\n",
    "        print('processing {}'.format(song))\n",
    "        try:\n",
    "            song_melody = melodies[song]\n",
    "            song_words = songsL[song]\n",
    "            song_X = []\n",
    "            song_y = []\n",
    "            for index, word in enumerate(song_words[:-1]):\n",
    "                try:\n",
    "                    word_embedding = E[word]\n",
    "                except Exception as e:\n",
    "                    word_embedding = np.zeros(300)\n",
    "                word_embedding_and_melody = add_melody_for_word(word_embedding, song_melody,index, strategy)\n",
    "                following_word = song_words[index + 1]\n",
    "                following_word_onehot = L_one_hot[following_word]\n",
    "                song_X.append([word_embedding_and_melody])\n",
    "                song_y.append(following_word_onehot)\n",
    "            songs_X += song_X\n",
    "            songs_Y += song_y\n",
    "            \n",
    "                \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    songs_X = np.vstack(songs_X)\n",
    "    songs_X = songs_X.reshape((songs_X.shape[0],1,songs_X.shape[1]))\n",
    "    return songs_X, songs_Y\n",
    "            \n",
    "                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DL MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_sampling(preds):\n",
    "    # we took this function from  \n",
    "    # The book Deep-Learning-Natural-Language-Processing/dp/1838550291 , page 183.\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) \n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    chosen =  np.argmax(probas)\n",
    "    return chosen\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_random_word(language, E):\n",
    "    #choose random word with existing embedding\n",
    "    try:\n",
    "        word = random.choice(language)\n",
    "        embedding = E[word]\n",
    "        if embedding is not None:\n",
    "            return word\n",
    "        else:\n",
    "            return choose_random_word(language, E)\n",
    "    except Exception: \n",
    "        return choose_random_word(language, E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_lstm_net(one_hot_embedding,one_hot_words,L,L_size, trian_melody_embeddings,E,songsL,L_one_hot, strategy):\n",
    "    if strategy == 1:\n",
    "        print(\"working on strategy 1\")\n",
    "        try:\n",
    "            model = load_model('{}\\strategy{}_model_sparse.h5'.format(env,strategy))\n",
    "            print('loaded model')\n",
    "            return model, 1\n",
    "        except Exception:\n",
    "            print(\"creating model for strategy 1\")\n",
    "            model = Sequential()\n",
    "            model.add(LSTM(128, input_shape=(1, 400)))\n",
    "            #model.add(LSTM(64,return_sequences=True))\n",
    "            #model.add(LSTM(32))\n",
    "            model.add(Dropout(0.2))\n",
    "            model.add(Dense(L_size))\n",
    "            model.add(Activation('softmax'))\n",
    "            optimizer = Adam(lr=0.0006)\n",
    "            model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['acc'])\n",
    "        return model, 0\n",
    "    if strategy == 2:\n",
    "        print(\"working on strategy 2\")\n",
    "        try:\n",
    "            model = load_model('{}\\strategy{}_model_sparse.h5'.format(env, strategy))\n",
    "            print('loaded model')\n",
    "            return model, 1\n",
    "        except Exception:\n",
    "            print(\"creating model for strategy 2\")\n",
    "            model = Sequential()\n",
    "            model.add(LSTM(128, input_shape=(1, 300 + 12)))\n",
    "            #model.add(LSTM(64,return_sequences=True))\n",
    "            #model.add(LSTM(32))\n",
    "            model.add(Dropout(0.2))\n",
    "            model.add(Dense(L_size))\n",
    "            model.add(Activation('softmax'))\n",
    "            optimizer = Adam(lr=0.0006)\n",
    "            model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['acc'])\n",
    "        return model, 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_songs(model,language,E,one_hot_words,melody_embeddings,strategy,random_words):\n",
    "    print(\"\\n\\n\\n\\n==================DONE TRAINING - validation=====================\\n\")\n",
    "    songs_words = get_songs_words(r'{}\\lyrics_test_set.csv'.format(env))\n",
    "    songs = list(songs_words.keys())\n",
    "    first_random_word = random_words[0]\n",
    "    second_random_word = random_words[1]\n",
    "    third_random_word = random_words[2]\n",
    "\n",
    "    for song_name in songs:\n",
    "        song_first_word = songs_words[song_name][0]\n",
    "        chosen_words = [song_first_word,first_random_word,second_random_word,third_random_word]\n",
    "        for real_or_random_index, word in enumerate(chosen_words):\n",
    "            song_created = word\n",
    "            print('working on word {} at song {}'.format(word, song_name))\n",
    "            song_first_word_embedding = E[word]\n",
    "            if strategy == 1:\n",
    "                song_first_word_match_melody = melody_embeddings[song_name][:100]\n",
    "            if strategy == 2:\n",
    "                song_first_word_match_melody = melody_embeddings[song_name][0]\n",
    "            first_input = np.concatenate([song_first_word_embedding,song_first_word_match_melody])\n",
    "            first_input = first_input.reshape((1,1,first_input.shape[0]))\n",
    "    \n",
    "            counter = 1\n",
    "            prefix = first_input\n",
    "            last_word = ''\n",
    "            # generate song with the same length as the original song\n",
    "            last_index = 0\n",
    "            while counter < len(songs_words[song_name]):\n",
    "                preds = model.predict(prefix[-1].reshape((1,) + prefix[-1].shape), verbose=0)[0]\n",
    "                preds[last_index] = 0\n",
    "                next_index = generate_sampling(preds)\n",
    "                last_index = next_index\n",
    "                next_word = one_hot_words[next_index]\n",
    "                song_created += ' ' + next_word\n",
    "                try:\n",
    "                    next_word = E[next_word]\n",
    "                except Exception:\n",
    "                    next_word =  np.zeros(300)\n",
    "                next_word =  add_melody_for_word(next_word, melody_embeddings[song_name],counter,strategy)\n",
    "                next_word = next_word.reshape((1,1,next_word.shape[0]))\n",
    "                counter = counter + 1\n",
    "                prefix = np.vstack((prefix, next_word))\n",
    "            docB = song_created\n",
    "            docA = songs_words[song_name]\n",
    "            similarity = get_similarity(docA,docB)\n",
    "            similarity2 = get_similarity2(docA,docB)\n",
    "            similarity3 = jaccard_similarity(docA, docB)\n",
    "            if real_or_random_index == 0:\n",
    "                selected = 'real'\n",
    "            else:\n",
    "                selected = 'random'\n",
    "            file_name = '{}\\{}_{}_{}-{}.txt'.format(env,selected,song_name,'S'+str(strategy),str(word))\n",
    "            with open(file_name, 'w+',encoding='utf-8') as res_file:\n",
    "                res = str(song_created).replace('&', '\\n')\n",
    "                res_file.write(\"similarity score : {} \\n\".format(similarity))\n",
    "                res_file.write(\"similarity_2 score : {} \\n\".format(similarity2))\n",
    "                res_file.write(\"similarity_jaccard score : {} \\n\".format(similarity3))\n",
    "                res_file.write(res)\n",
    "            \n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity(textA, textB):\n",
    "    NLP = spacy.load('en_core_web_lg')\n",
    "    textA = ' '.join(textA)\n",
    "    textA = textA.replace('&', '\\n')\n",
    "    textB = textB.replace('&', '\\n')\n",
    "    docA = NLP(u'{}'.format(textA))\n",
    "    docB = NLP(u'{}'.format(textB))\n",
    "    print(docA)\n",
    "    print('-------------------')\n",
    "    print(docB)\n",
    "    similarity = docA.similarity(docB)\n",
    "    print('generated and original song similarity is: {}'.format(similarity))\n",
    "    return similarity\n",
    "\n",
    "\n",
    "def jaccard_similarity(textA, textB):\n",
    "    textA = ' '.join(textA)\n",
    "    textA = textA.replace('&', ' ')\n",
    "    textB = textB.replace('&', ' ')\n",
    "    list1 = textA.split(\" \")\n",
    "    list2 = textB.split(\" \")\n",
    "    s1 = set(list1)\n",
    "    s2 = set(list2)\n",
    "    return len(s1.intersection(s2)) / len(s1.union(s2))\n",
    "\n",
    "\n",
    "\n",
    "def get_similarity2(t1,t2):\n",
    "    t1 = ' '.join(t1)\n",
    "    t1 = t1.replace('&', '')\n",
    "    t2 = t2.replace('&', '')\n",
    "    print(t1)\n",
    "    print('-----')\n",
    "    print(t2)\n",
    "\n",
    "    # tokenization \n",
    "    t1_list = word_tokenize(t1)  \n",
    "    t2_list = word_tokenize(t2) \n",
    "  \n",
    "    # sw contains the list of stopwords \n",
    "    sw = stopwords.words('english')  \n",
    "    l1 =[];l2 =[] \n",
    "  \n",
    "    # remove stop words from string \n",
    "    t1_set = {w for w in t1_list if not w in sw}  \n",
    "    t2_set = {w for w in t2_list if not w in sw} \n",
    "\n",
    "    # form a set containing keywords of both strings  \n",
    "    rvector = t1_set.union(t2_set)  \n",
    "    for w in rvector: \n",
    "        if w in t1_set: l1.append(1) # create a vector \n",
    "        else: l1.append(0) \n",
    "        if w in t2_set: l2.append(1) \n",
    "        else: l2.append(0) \n",
    "    c = 0\n",
    "\n",
    "    # cosine formula  \n",
    "    for i in range(len(rvector)): \n",
    "            c+= l1[i]*l2[i] \n",
    "    cosine = c / float((sum(l1)*sum(l2))**0.5) \n",
    "    print(\"similarity_2: \", cosine) \n",
    "    return cosine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = 'C:\\\\Users\\\\idan\\\\Desktop\\\\ass3'\n",
    "path = r'{}\\lyrics_train_set.csv'.format(env)  \n",
    "trian_melody_embeddings_s1= generate_melody_vecors(path, 'midi_train_set_S1_100',1)\n",
    "path = r'{}\\lyrics_test_set.csv'.format(env)  \n",
    "test_melody_embeddings_s1 = generate_melody_vecors(path, 'midi_test_set_S1_100',1)\n",
    "melody_embeddings_s1 = {}\n",
    "melody_embeddings_s1.update(trian_melody_embeddings_s1)\n",
    "melody_embeddings_s1.update(test_melody_embeddings_s1)\n",
    "dump_pickle(r'{}\\{}.pickle'.format(env,'all_melodies_S1_100'), melody_embeddings_s1)\n",
    "\n",
    "\n",
    "path = r'{}\\lyrics_train_set.csv'.format(env)  \n",
    "trian_melody_embeddings_s2= generate_melody_vecors(path, 'midi_train_set_S2_12',2)\n",
    "path = r'{}\\lyrics_test_set.csv'.format(env)  \n",
    "test_melody_embeddings_s2 = generate_melody_vecors(path, 'midi_test_set_S2_12',2)\n",
    "melody_embeddings_s2 = {}\n",
    "melody_embeddings_s2.update(trian_melody_embeddings_s2)\n",
    "melody_embeddings_s2.update(test_melody_embeddings_s2)\n",
    "dump_pickle(r'{}\\{}.pickle'.format(env,'all_melodies_S2_12'), melody_embeddings_s2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'{}\\lyrics_train_set.csv'.format(env)\n",
    "env = r'C:\\Users\\idan\\Desktop\\ass3'\n",
    "#glove-wiki-gigaword-300 & ord2vec-google-news-300\n",
    "print('generate one hot embedding')\n",
    "one_hot_embedding = generate_one_hot_embedding(path)\n",
    "print('generate one hot words')\n",
    "one_hot_words = generate_one_hot_words(path)\n",
    "a = r'word2vec-google-news-300'\n",
    "E = download_embeddings(a)\n",
    "songsL = get_songs_words(path)\n",
    "language = extract_language(path)\n",
    "L_size = len(language)\n",
    "language = list(language)\n",
    "L = np.array(language)\n",
    "L_one_hot = generate_onehot_for_language(L)\n",
    "one_hot_words = generate_one_hot_words(path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_words = []\n",
    "for i in range(3):\n",
    "    random_words.append(choose_random_word(language, E))       \n",
    "for strategy in [1,2]:\n",
    "    print('working on strategy {}'.format(strategy))\n",
    "    model_status = 0\n",
    "    if strategy == 1:\n",
    "        model, model_status= generate_lstm_net(one_hot_embedding,one_hot_words,L,L_size, trian_melody_embeddings_s1,E,songsL,L_one_hot,strategy) \n",
    "        X_train,y_train =  generate_dataset(trian_melody_embeddings_s1, E, songsL, L, L_one_hot, strategy)\n",
    "        melody_embeddings = melody_embeddings_s1\n",
    "    if strategy == 2:\n",
    "        model, model_status = generate_lstm_net(one_hot_embedding,one_hot_words,L,L_size, trian_melody_embeddings_s2,E,songsL,L_one_hot, strategy)\n",
    "        X_train,y_train =  generate_dataset(trian_melody_embeddings_s2, E, songsL, L, L_one_hot, strategy)\n",
    "        melody_embeddings = melody_embeddings_s2\n",
    "    print('model status is: {}'.format(model_status))\n",
    "    X_train,y_train = np.array(X_train), np.array(y_train)\n",
    "    tensorboard = TensorBoard(log_dir=r'{}\\tensorboard_strategy_random_words{}\\{}'.format(env,strategy,time.time()))\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=35)\n",
    "    print(\"creating balanced weights\")\n",
    "    num_labels = np.argmax(y_train, axis=1)\n",
    "    class_weights = compute_class_weight('balanced', np.unique(num_labels), np.array(num_labels))\n",
    "    #d_class_weights = dict(enumerate(class_weights))\n",
    "    if model_status == 0 :\n",
    "        model.fit(X_train, y_train, epochs=50, verbose=1, validation_split=0.2,\n",
    "              callbacks=[tensorboard,early_stopping], class_weight=class_weights) \n",
    "        print('SAVING MODEL - version {}'.format(strategy))\n",
    "        model.save('{}\\strategy{}_model_random_words.h5'.format(env,strategy)) \n",
    "    else:\n",
    "        print('model alreay loaded')\n",
    "        \n",
    "    create_songs(model,language,E,one_hot_words,melody_embeddings,strategy,random_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
